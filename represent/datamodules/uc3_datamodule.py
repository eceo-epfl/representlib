"""
This code is generated by Ridvan Salih KUZU
LAST EDITED:  20.02.2023
ABOUT SCRIPT:
It defines Data Loader class and functions
"""

import torch
from torch.utils.data import Dataset
import pandas as pd
import numpy as np
import copy
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from tslearn.preprocessing import TimeSeriesScalerMeanVariance,TimeSeriesScalerMinMax
from scipy import signal



class DataReader(Dataset):
    def __init__(self, unique_array, data_array, footprint_array, anomaly_array=None,is_permute=True):

       self.data_array=data_array
       self.time_dimension=data_array.shape[0]
       self.footprint_array=footprint_array
       self.anomaly_array=anomaly_array
       self.unique_id_array=unique_array
       self.is_permute=is_permute

    def __len__(self):
        return self.data_array.shape[1]

    def __getitem__(self, index):

        data = self.data_array[:,index,:]

        if self.is_permute:
            permute_indices = torch.randperm(self.time_dimension)
            permuted_data =  data[permute_indices, :]
        else:
            permuted_data=data

        foot_print=self.footprint_array[index]
        unique_id=self.unique_id_array[index]

        if isinstance(self.anomaly_array, type(None)):
            return data.astype(np.float32), permuted_data.astype(np.float32), unique_id.astype(int), foot_print.astype(int), np.zeros(foot_print.shape).astype(int)
        else:
            anomaly_type = self.anomaly_array[index]
            return data.astype(np.float32), permuted_data.astype(np.float32), unique_id.astype(int), foot_print.astype(int), anomaly_type.astype(int)





class Dataloader():
    """
    THIS CLASS ORCHESTRATES THE TRAINING, VALIDATION, AND TEST DATA GENERATORS
    """
    def __init__(self,
                 train_dir,
                 test_dir,
                 eval_dir,
                 split=0.25,
                 batch_size=32,
                 num_workers=1,
                 window_length=9,
                 is_normalize=True,
                 is_detrend=True,
                 is_permute=True,
                 is_gradient=False,
                 is_debug=False,
                 seed=42,
                 ):

        def _derivator(data,is_gradient):
            if is_gradient:
                p_data = np.gradient(data, axis=0)
                pp_data = np.gradient(data, axis=0)
                data = np.concatenate([data, p_data, pp_data], -1)
                return data
            else:
                return data

        df_train = pd.read_csv(train_dir)
        df_test = pd.read_csv(test_dir)
        df_eval = pd.read_csv(eval_dir)
        df_train = df_train.fillna(0)
        df_test = df_test.fillna(0)
        df_eval = df_eval.fillna(0)



        df_train, df_val = train_test_split(df_train, test_size=split, random_state=seed)

        fp_train, ts_train = Dataloader._create_feature_vector(df_train, building=False, synthetic=False)
        fp_val, ts_val = Dataloader._create_feature_vector(df_val, building=False, synthetic=False)
        fp_test, ts_test = Dataloader._create_feature_vector(df_test, building=False, synthetic=False)
        fp_eval, ts_eval, anomaly_id = Dataloader._create_feature_vector(df_eval, building=False, synthetic=True)
        if is_debug:
            ts_train=ts_train[:,:1024,:]
            ts_val = ts_val[:, :1024, :]
            ts_test = ts_test[:, :1024, :]
            ts_eval = ts_eval[:, :1024, :]

        ts_train = np.transpose(ts_train, (1, 0, 2))
        ts_val = np.transpose(ts_val, (1, 0, 2))
        ts_test = np.transpose(ts_test, (1, 0, 2))
        ts_eval = np.transpose(ts_eval, (1, 0, 2))

        if window_length>1:
            ts_train = Dataloader._smooth_multiple(ts_train, window_len=window_length)
            ts_val = Dataloader._smooth_multiple(ts_val, window_len=window_length)
            ts_test = Dataloader._smooth_multiple(ts_test, window_len=window_length)
            ts_eval = Dataloader._smooth_multiple(ts_eval, window_len=window_length)

        if is_detrend:
            ts_train =Dataloader._detrend_multiple(ts_train)
            ts_val = Dataloader._detrend_multiple(ts_val)
            ts_test = Dataloader._detrend_multiple(ts_test)
            ts_eval = Dataloader._detrend_multiple(ts_eval)

        if is_normalize:
            scaler = TimeSeriesScalerMeanVariance()
            scaler.fit(ts_train)
            ts_train = scaler.transform(ts_train)
            ts_val = scaler.transform(ts_val)
            ts_test = scaler.transform(ts_test)
            ts_eval = scaler.transform(ts_eval)

        ts_train = np.transpose(ts_train, (1, 0, 2))
        ts_val = np.transpose(ts_val, (1, 0, 2))
        ts_test = np.transpose(ts_test, (1, 0, 2))
        ts_eval = np.transpose(ts_eval, (1, 0, 2))

        ts_train=_derivator(ts_train,is_gradient)
        ts_val=_derivator(ts_val,is_gradient)
        ts_test=_derivator(ts_test,is_gradient)
        ts_eval=_derivator(ts_eval,is_gradient)

        self.ts_test=ts_test
        self.ts_eval = ts_eval
        self.dimension = ts_train.shape #timeDimensionTrain, trainingDataSize, featureVectorSize


        train_dataset = DataReader(df_train.index.values,ts_train, fp_train,is_permute=is_permute)
        val_dataset = DataReader(df_val.index.values,ts_val, fp_val,is_permute=is_permute)
        test_dataset = DataReader(df_test.index.values, ts_test, fp_test,is_permute=is_permute)
        eval_dataset = DataReader(df_eval.index.values,ts_eval, fp_eval, anomaly_id,is_permute)

        self.dataloaders = {}

        self.dataloaders['train'] = torch.utils.data.DataLoader( train_dataset,batch_size=batch_size,
                                                           shuffle=True, num_workers=num_workers, pin_memory=True)
        self.dataloaders['valid'] = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,
                                                           shuffle=True, num_workers=num_workers, pin_memory=True)
        self.dataloaders['test'] = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,
                                                           shuffle=False, num_workers=num_workers, pin_memory=False)
        self.dataloaders['eval'] = torch.utils.data.DataLoader( eval_dataset, batch_size=batch_size,
                                                              shuffle=False, num_workers=num_workers, pin_memory=False)

    def get_eval_array(self):
        return self.ts_eval

    def get_test_array(self):
        return self.ts_test

    def get_data_loader(self, type):
        return self.dataloaders[type]

    def get_data_dimensions(self):
        return self.dimension

    @staticmethod
    def _smooth(x, window_len=11, window='hanning'):
        """smooth the data using a window with requested size.

        This method is based on the convolution of a scaled window with the signal.
        The signal is prepared by introducing reflected copies of the signal
        (with the window size) in both ends so that transient parts are minimized
        in the begining and end part of the output signal.

        input:
            x: the input signal
            window_len: the dimension of the smoothing window; should be an odd integer
            window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'
                flat window will produce a moving average smoothing.

        output:
            the smoothed signal

        example:

        t=linspace(-2,2,0.1)
        x=sin(t)+randn(len(t))*0.1
        y=smooth(x)

        see also:

        numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve
        scipy.signal.lfilter

        TODO: the window parameter could be the window itself if an array instead of a string
        NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.
        """

        if x.ndim != 1:
            raise ValueError("smooth only accepts 1 dimension arrays.")

        if x.size < window_len:
            raise ValueError("Input vector needs to be bigger than window size.")

        if window_len < 3:
            return x

        if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:
            raise ValueError("Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'")

        s = np.r_[x[window_len - 1:0:-1], x, x[-2:-window_len - 1:-1]]
        # print(len(s))
        if window == 'flat':  # moving average
            w = np.ones(window_len, 'd')
        else:
            w = eval('np.' + window + '(window_len)')

        y = np.convolve(w / w.sum(), s, mode='valid')
        return y[:len(x)]

    @staticmethod
    def _smooth_multiple(x, window_len=11, window='hanning'):
        xc = copy.deepcopy(x)
        dims = xc.shape
        for i in range(dims[0]):
            xc[i, :, 0] = Dataloader._smooth(xc[i, :, 0], window_len, window)
        return xc

    @staticmethod
    def _detrend_multiple(x):
        xc = copy.deepcopy(x)
        dims = xc.shape
        for i in range(dims[0]):
            xc[i, :, 0] = signal.detrend(xc[i, :, 0])
        return xc

    @staticmethod
    def _create_feature_vector(df, pad_num=10, building=True, synthetic=True):
        """ group PSs of each building to a feature vector, pad/shrink to 10 points"""
        array_fp = np.array(df['footprint_id'])
        ftp_ids, counts = np.unique(array_fp, return_counts=True)
        np_timeseries_all = []
        if building == True:
            for ftp_id in ftp_ids:
                # ps_ids = df.index[df.footprint_id == ftp_id]
                # df_index = df.iloc[ps_ids]
                df_index = df[df.footprint_id == ftp_id]
                np_timeseries = df_index.filter(regex="20*").values
                # np_timeseries = signal.detrend(np_timeseries) ## Remove linear trend

                if np_timeseries.shape[0] < pad_num:
                    to_pad = pad_num - np_timeseries.shape[0]
                    # np_timeseries = np.pad(np_timeseries,((0,to_pad),(0,0))) ## v1: pad 0
                    np_timeseries = np.pad(np_timeseries, ((0, to_pad), (0, 0)), mode='mean')  ## v2: pad mean
                else:
                    # np_timeseries = np_timeseries[:pad_num,:] ## v1: first 10 values
                    kmeans = KMeans(n_clusters=pad_num).fit(np_timeseries)
                    np_timeseries = kmeans.cluster_centers_  ## v2: kmeans cluster 10 values

                # np_timeseries = np_timeseries.reshape(np_timeseries.shape[1], 1, np_timeseries.shape[0])
                np_timeseries = np.transpose(np_timeseries, (1, 0))
                np_timeseries = np.expand_dims(np_timeseries, 1)
                np_timeseries_all.append(np_timeseries.astype('float32'))

            return ftp_ids, np.concatenate(np_timeseries_all, axis=1)  # (seq_len, batch_size, input_size)
        else:
            np_timeseries = df.filter(regex="20*").values
            np_timeseries = np.transpose(np_timeseries, (1, 0))
            np_timeseries = np.expand_dims(np_timeseries, -1)
            # np_timeseries_all.append(np_timeseries.astype('float32'))
            if synthetic:
                return np.array(df['footprint_id']), np_timeseries, df.anomaly.values
            else:
                return np.array(df['footprint_id']), np_timeseries


