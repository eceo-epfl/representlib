"""
This code is generated by Ridvan Salih KUZU
LAST EDITED:  20.03.2023
ABOUT SCRIPT:
It runs learning on given Controstive model and makes hyperparameter fine-tuning
"""

import torch
import torch.nn as nn
from represent.models.uc1_byol import BYOL
from represent.models.uc1_resnet_base import ResNetFc
from represent.tools.utils_uc1 import AvgMeter,get_lr
from represent.datamodules.uc1_contrastive_datamodule import DataLoader
from tqdm import tqdm
import optuna
from optuna.samplers import TPESampler
import argparse
import joblib
import gc
import os
import copy
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.simplefilter("ignore")

def main(args):
    if not os.path.exists(args.log_dir):
        os.makedirs(args.log_dir)

    accelerator = "cuda" if torch.cuda.is_available() else "cpu"
    device = torch.device(accelerator)


    resnet = ResNetFc(resnet_name="ResNet18", model_path=args.model_dir + "/resnet18-base.pth",
                      num_channels=args.channel_size)


    #model = nn.DataParallel(model, device_ids=[i for i in range(torch.cuda.device_count())])
    #torch.save(model.online_encoder.net.state_dict(), 'model_best.pth.tar'.format())
    data_loader=DataLoader(args.database_dir,args.input_type,args.image_size,args.is_reduced,batch_size=args.batch_size,num_workers=args.num_workers,data_filter=args.data_filter)

    def objective(trial):
        '''
            THIS FUNCTION DEFINES THE OBJECTIVE FUNCTION FOR BAYESIAN HYPERPARAMETER TUNING
            :param trial: trial object of bayesian optimization
            :return: returns loss score to be minimized
        '''

        gc.collect()

        print(f"INFO: Trial number: {trial.number}\n")

        learning_rate = trial.suggest_categorical('learning_rate', args.learning_rate)
        penalty_rate = trial.suggest_categorical('penalty_rate', args.penalty_rate)
        sim_siam = trial.suggest_categorical('sim_siam', args.sim_siam)



        output_file_template = '{}/S{}_L{}_P{}_S{}_N{}'.format(args.log_dir, args.input_type,learning_rate, penalty_rate,sim_siam,trial.number)

        model = BYOL(
            resnet,
            image_size=args.image_size,
            channel_size=args.channel_size,
            projection_size=args.image_size,  # size of projection output, 256 was used in the paper
            projection_hidden_size=2048,  # size of projection hidden dimension, paper used 2048
            moving_average_decay=0.99,  # exponential moving average decay of target encoder
            hidden_layer='avgpool' if sim_siam==1 else -2,
            use_momentum=False if sim_siam==1 else True
        ).to(device)

        best_model, best_loss = train_validate(model, data_loader.get_data_loader('train'), data_loader.get_data_loader('valid'), learning_rate,penalty_rate,sim_siam==1, output_file_template, device)

        return best_loss

    study = optuna.create_study(sampler=TPESampler(), direction='minimize',
                                pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=24, interval_steps=6),)

    log_file=args.log_dir+'optimization_logs.pkl'
    if os.path.isfile(log_file):
        study = joblib.load(log_file)

    study.optimize(objective, n_trials=args.n_trials, gc_after_trial=True)
    #joblib.dump(study, log_file)




def train_epoch(model, train_loader, optimizer, lr_scheduler,sim_siam, step, device):
    '''
         THIS FUNCTION RUNS THE SINGLE TRAINING EPOCHS
         :param model: CLIP model to be fine-tuned
         :param train_loader: data loader for training files
         :param optimizer: model optimizer object
         :param lr_scheduler: scheduler object to update the learning rate for optimization
         :param step: epoch step number
         :param device: GPU or CPU device
         :return: returns average training loss
    '''

    loss_meter = AvgMeter()
    tqdm_object = tqdm(train_loader, total=len(train_loader))
    for image_batch in tqdm_object:
        loss = model(image_batch.to(device).type(torch.float32))
        optimizer.zero_grad()
        loss.backward()
        if not sim_siam:
            model.update_moving_average()
        optimizer.step()
        if step == "batch":
            lr_scheduler.step()

        count = image_batch.size(0)
        loss_meter.update(loss.item(), count)

        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))
    return model


def valid_epoch(model, valid_loader, device):
    '''
             THIS FUNCTION RUNS THE SINGLE TRAINING EPOCHS
             :param model: CLIP model to be fine-tuned
             :param valid_loader: data loader for validation files
             :param device: GPU or CPU device
             :return: returns the tuple of ( average validation loss, an image batch example, a text batch example).
                      image and text batches are used as template to save the model if it reaches the optimum
        '''

    loss_meter = AvgMeter()
    tqdm_object = tqdm(valid_loader, total=len(valid_loader))
    for image_batch in tqdm_object:
        loss = model(image_batch.to(device).type(torch.float32))

        count = image_batch.size(0)
        loss_meter.update(loss.item(), count)

        tqdm_object.set_postfix(valid_loss=loss_meter.avg)
    return loss_meter, image_batch.to(device).type(torch.float32)



def train_validate(model, train_loader, valid_loader, learning_rate,penalty_rate, sim_siam, output_template, device):
    '''
        THIS FUNCTION MANAGES THE EXECUTION OF TRAINING AND VALIDATION EPOCHS
        :param model: CLIP model to be fine-tuned
        :param train_loader: data loader for training files
        :param valid_loader: data loader for validation files
        :param learning_rate: learning rate for optimization
        :param downstream_task: one of few-shot learning tasks 'DEFAULT', 'ARC_HEAD','MLP_HEAD','ARC_MLP_HEAD'
        :param output_template: file name template to save the fine-tuned models and inference statistics
        :param device: GPU or CPU device
        :return:
    '''

    best_model=None
    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), betas=(0.9,0.98),eps=1e-6, lr=learning_rate,weight_decay=penalty_rate)  # weight_decay=0.0001
    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)

    step = "epoch"
    best_loss = float('inf')
    for epoch in range(args.total_epoch):
        print(f"Epoch: {epoch + 1}")
        model.train()

        model=train_epoch(model, train_loader, optimizer, lr_scheduler,sim_siam, step, device)
        model.eval()
        with torch.no_grad():
            loss_meter, image_batch = valid_epoch(model, valid_loader,device)


        if loss_meter.avg < best_loss:
            best_loss = loss_meter.avg
            best_model = copy.deepcopy(model)
            #torch.jit.save(torch.jit.trace(best_model.online_encoder.net, image_batch),'{}_model_best.pth.tar'.format(output_template))
            torch.save(best_model.online_encoder.net, '{}_{}_model_best.pth'.format(output_template,epoch+1))
            print("INFO: Best Model saved into " + '{}_{}_model_best.pth.tar'.format(output_template,epoch+1))

        lr_scheduler.step(loss_meter.avg)
    return best_model,best_loss



if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument('--database-dir', type=str, default='represent/data/UC1/contrastive_learning/')
    #parser.add_argument('--database-dir', type=str, default='/local_home/kuzu_ri/GIT_REPO/iforest/data/switzerland/2015/')
    parser.add_argument('--model-dir', default='represent/weights/', type=str,help='Model Directory (default: represent/weights)')

    parser.add_argument('--image-size', type=int, default=512)
    parser.add_argument('--sim-siam', type=int, nargs='+', default=[1,0])
    parser.add_argument('--channel-size', type=int, default=4)
    parser.add_argument('--is-reduced', type=bool, default=True)
    parser.add_argument('--input-type', type=int, default=2)
    parser.add_argument('--data-filter', type=str, default='s2_l1c')
    parser.add_argument('-b', '--batch-size', default=32, type=int, metavar='BS',help='number of batch size (default: 32)')
    parser.add_argument('--log-dir', type=str, default='represent/results/contrastive_learning/')
    parser.add_argument('--total-epoch', type=int, default=15)
    parser.add_argument('--num-workers', type=int, default=8)
    parser.add_argument('--n-trials', type=int, default=1)
    parser.add_argument('--learning-rate', type=float, nargs='+', default=[1e-3, 5e-4])
    parser.add_argument('--penalty-rate', type=float, nargs='+', default=[5e-6])



    args = parser.parse_args()

    main(args)



