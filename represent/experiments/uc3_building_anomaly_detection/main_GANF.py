"""
This code is generated by Ridvan Salih KUZU
LAST EDITED:  20.02.2023
ABOUT SCRIPT:
It defines training and evaluation functions
"""

import sys
sys.path.append("../..")
#import sys; sys.argv=['']; del sys
from tqdm import tqdm
import optuna
from optuna.samplers import TPESampler
import argparse
import warnings
import joblib
import gc
import numpy as np
from represent.tools.utils import  AvgMeter, get_lr, anomaly_score_reports,frame_anomaly_table
from represent.datamodules.uc3_datamodule import Dataloader
from represent.models.uc3_benchmark.ganf.ganf import GANF
import os
from torch.nn.init import xavier_uniform_
from torch.nn.utils import clip_grad_value_

import torch

warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)


def train_validate(model, data_loader,learning_rate, penalty_rate, output_file_template, rho=1,rho_max=1e16,alpha=0,max_iter=10,h_tol=1e-4,A=None,step_save=False,device='cpu'):
    '''
        THIS FUNCTION MANAGES THE EXECUTION OF TRAINING AND VALIDATION EPOCHS
        :param model: LSTM model to be trained or fine-tuned
        :param data_loader: data loader
        :param learning_rate: learning rate for optimization
        :param penalty_rate: penalty rate for optimization
        :param output_file_template: file name template to save the fine-tuned uc1_resnet_base.py and inference statistics
        :param device: GPU or CPU device
        :return:
    '''

    best_model=None
    h_A_old = np.inf
    for _ in range(max_iter):
        while rho < rho_max:
            optimizer = torch.optim.Adam([
                {'params': model.parameters(), 'weight_decay': penalty_rate},
                {'params': [A]}], lr=learning_rate, weight_decay=0.0)
            #optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), betas=(0.9,0.98),eps=1e-6, lr=learning_rate,weight_decay=penalty_rate)  # weight_decay=0.0001
            #optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)  # model.parameters is the parameters that optimizerd have to change

            lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)


            step = "epoch"
            best_loss = float('inf')
            for epoch in range(args.total_epoch):
                print(f"Epoch: {epoch + 1}")
                model.train()
                _, h=train_epoch(model, data_loader.get_data_loader('train'), optimizer, lr_scheduler, step, device,A,rho,alpha)
                model.eval()
                with torch.no_grad():
                    valid_loss, image_batch = valid_epoch(model, data_loader.get_data_loader('valid'), device,A)
                    if step_save: test_evaluate(model,A, data_loader, output_file_template, device,epoch)


                if valid_loss.avg < best_loss:
                    best_loss = valid_loss.avg
                    #best_model = copy.deepcopy(model)
                    #torch.jit.save(torch.jit.trace(model, (image_batch)),'{}_model_best.pth.tar'.format(output_file_template))
                    torch.save(A.data, '{}_model_A.pth.tar'.format(output_file_template))
                    torch.save(model.state_dict(), '{}_model_best.pth.tar'.format(output_file_template))
                    print("INFO: Best Model saved into " + '{}_model_best.pth.tar'.format(output_file_template))

                lr_scheduler.step(valid_loss.avg)

            del optimizer
            torch.cuda.empty_cache()

            if h.item() > 0.5 * h_A_old:
                rho *= 10
            else:
                break

        h_A_old = h.item()
        alpha += rho * h.item()

        if h_A_old <= h_tol or rho >= rho_max:
            break


    return best_model

def train_epoch(model, train_loader, optimizer, lr_scheduler, step, device,A,rho,alpha):
    '''
         THIS FUNCTION RUNS THE SINGLE TRAINING EPOCHS
         :param model: LSTM model to be trained or fine-tuned
         :param train_loader: data loader for training files
         :param optimizer: model optimizer object
         :param lr_scheduler: scheduler object to update the learning rate for optimization
         :param loss_criterion: loss objective function to be utilized for backprop
         :param step: epoch step number
         :param device: GPU or CPU device
         :return: returns average training loss
    '''

    loss_meter = AvgMeter()
    tqdm_object = tqdm(train_loader, total=len(train_loader))
    for data_batch, permuted_data_batch, unique_id_batch, foot_batch, _ in tqdm_object:
        optimizer.zero_grad()
        loss = -model(permuted_data_batch.to(device), A)
        h = torch.trace(torch.matrix_exp(A * A)) - 1
        total_loss = loss + 0.5 * rho * h * h + alpha * h
        total_loss.backward()
        clip_grad_value_(model.parameters(), 1)
        optimizer.step()
        if step == "batch":
            lr_scheduler.step()

        count = data_batch.size(1)
        loss_meter.update(loss.item(), count)
        A.data.copy_(torch.clamp(A.data, min=0, max=1))
        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))
    return loss_meter,h


def valid_epoch(model, valid_loader, device,A):
    '''
         THIS FUNCTION RUNS THE SINGLE TRAINING EPOCHS
         :param model: LSTM model to be trained or fine-tuned
         :param valid_loader: data loader for validation files
         :param loss_criterion: loss objective function to be utilized for backprop
         :param device: GPU or CPU device
         :return: returns the tuple of ( average validation loss, an image batch example, a text batch example).
                  image and text batches are used as template to save the model if it reaches the optimum
    '''

    loss_meter = AvgMeter()

    tqdm_object = tqdm(valid_loader, total=len(valid_loader))
    for data_batch, permuted_data_batch, unique_id_batch, foot_batch, _ in tqdm_object:
        loss = -model(permuted_data_batch.to(device), A)

        count = data_batch.size(1)
        loss_meter.update(loss.item(), count)

        tqdm_object.set_postfix(valid_loss=loss_meter.avg)
    return loss_meter, data_batch.to(device)


def test_evaluate(model,A, data_loader, output_file_template, device,step="final"):
    '''
           THIS FUNCTION MANAGES THE EXECUTION OF TRAINING AND VALIDATION EPOCHS
           :param model: LSTM model to be evaluated or tested
           :param data_loader: data loader
           :param penalty_rate: penalty rate for optimization
           :param loss_type: l1 or dtw to select the loss function
           :param output_file_template: file name template to save the fine-tuned uc1_resnet_base.py and inference statistics
           :param device: GPU or CPU device
           :return:
       '''


    model.eval()
    df_test=eval_epoch(model,A, data_loader.get_data_loader('test'), device)
    df_eval = eval_epoch(model,A, data_loader.get_data_loader('eval'), device)

    df_report, u, acc = anomaly_score_reports(df_test, df_eval, min_percentile=80, step_resolution=1,per_building=False)

    df_test.to_csv("{}_prediction_results_organic_{}.csv".format(output_file_template,step),index=False)
    df_eval.to_csv("{}_prediction_results_synthetic_{}.csv".format(output_file_template,step),index=False)
    df_report.to_csv("{}_acc_{}_u_{}_prediction_report_{}.csv".format(output_file_template,acc,u,step), index=False)

    return acc


def eval_epoch(model,A, data_loader, device):
    '''
        THIS FUNCTION MANAGES THE EXECUTION OF TRAINING AND VALIDATION EPOCHS
        :param model: CLIP model to be fine-tuned
        :param data_loader: data loader for evaluation file
        :param loss_criterion: loss objective function to be utilized for evaluation
        :param device: GPU or CPU device
        :return:
    '''

    loss_list=[]
    unique_list = []
    footprint_list = []
    anomaly_list = []
    with torch.no_grad():
        tqdm_object = tqdm(data_loader, total=len(data_loader))
        for data_batch, permuted_data_batch, unique_id_batch, foot_batch, anomaly_batch in tqdm_object:
            loss = -model.test(permuted_data_batch.to(device),A)
            loss_list.append(loss.detach().cpu().numpy())

            unique_list.append(unique_id_batch.detach().cpu().numpy())
            footprint_list.append(foot_batch.detach().cpu().numpy())
            anomaly_list.append(anomaly_batch.detach().cpu().numpy())
        loss_list = np.concatenate(loss_list, 0)
        loss_list = np.expand_dims(np.expand_dims(loss_list, -1), -1)

    df=frame_anomaly_table(loss_list, np.concatenate(unique_list), np.concatenate(footprint_list),  np.concatenate(anomaly_list))

    return df



def main(args):

    if not os.path.exists(args.log_dir):
        os.makedirs(args.log_dir,exist_ok=True)

    MANUAL_SEED=42
    LABEL_NAMES = ["trend", "noise", "step"]
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(device)
    torch.manual_seed(MANUAL_SEED)  # set seed (manually) for generate random numbers with CPU
    torch.cuda.manual_seed(MANUAL_SEED)  # set seed (manually) for generate random numbers with GPU --> "CUDA = GPU"
    np.random.seed(MANUAL_SEED)

    def objective(trial):
        '''
            THIS FUNCTION DEFINES THE OBJECTIVE FUNCTION FOR BAYESIAN HYPERPARAMETER TUNING
            :param trial: trial object of bayesian optimization
            :return: returns weighted F1 score to be maximized
        '''

        gc.collect()

        learning_rate = trial.suggest_categorical('learning_rate', args.learning_rate)
        smooth_window = trial.suggest_categorical('smooth_window', args.smooth_window)
        penalty_rate = trial.suggest_categorical('penalty_rate', args.penalty_rate)
        embedding_dim = trial.suggest_categorical('embedding_dim', args.embedding_dim)
        method = trial.suggest_categorical('method', args.method)

        is_normalize = args.is_normalize #trial.suggest_categorical('is_normalize', [True])
        is_detrend = args.is_detrend#trial.suggest_categorical('is_detrend', [False])
        is_permute=args.is_permute
        is_debug = args.is_debug
        is_gradient=args.is_gradient
        rho=args.rho_init
        rho_max=args.rho_max
        alpha=args.alpha_init
        max_iter=args.max_iter
        h_tol=args.h_tol


        print(f"INFO: Trial number: {trial.number}")
        print(f"INFO: Learning rate: {learning_rate}")
        print(f"INFO: Penalty rate: {penalty_rate}")
        print(f"INFO: Embedding dimension: {embedding_dim}")
        print(f"INFO: Smooth window: {smooth_window}")
        print(f"INFO: Is normalize: {is_normalize}")
        print(f"INFO: Is detrend: {is_detrend}")
        print(f"INFO: Method: {method}")

        print(f"INFO: Data Permutation: {is_permute}")
        print(f"INFO: Data Gradient: {is_gradient}")
        print(f"INFO: DEBUG mode: {is_debug}\n")

        data_loader = Dataloader(args.train_dir,
                                 args.test_dir,
                                 args.eval_dir,
                                 split=0.25,
                                 batch_size=args.batch_size,
                                 window_length=smooth_window,
                                 num_workers=args.num_workers,
                                 is_normalize=is_normalize,
                                 is_detrend=is_detrend,
                                 is_permute=is_permute,
                                 is_gradient=is_gradient,
                                 is_debug=is_debug,
                                 seed=MANUAL_SEED)


        output_file_template = '{}_lr_{}_pr_{}_sw_{}_in_{}_id_{}_ip_{}_ig_{}_me_{}_ed_{}_tn_{}'.format(args.log_dir,learning_rate,
                                                                                                                        penalty_rate,smooth_window,is_normalize,is_detrend,
                                                                                                                        is_permute,is_gradient,method,embedding_dim, trial.number)

        init = torch.zeros([data_loader.get_data_dimensions()[-1], data_loader.get_data_dimensions()[-1]])
        init = xavier_uniform_(init).abs()
        init = init.fill_diagonal_(0.0)
        A = torch.tensor(init, requires_grad=True, device=device)
        model=GANF(1, 1, embedding_dim, 1)


        model = model.to(device)
        train_validate(model, data_loader,learning_rate, penalty_rate, output_file_template,rho, rho_max, alpha, max_iter, h_tol,A,args.step_save,device)

        best_model = GANF(1, 1, embedding_dim, 1)
        best_model.load_state_dict(torch.load('{}_model_best.pth.tar'.format(output_file_template),device))
        best_model=best_model.to(device)
        init = torch.load('{}_model_A.pth.tar'.format(output_file_template)).to(device).abs()
        best_A=torch.tensor(init, requires_grad=False, device=device)

        acc=test_evaluate(best_model,best_A, data_loader, output_file_template, device)
        print(f"INFO: Accuracy @ 80 percentile: {acc}\n")
        return acc

    study = optuna.create_study(sampler=TPESampler(), direction='maximize',
                                pruner=optuna.pruners.MedianPruner(n_startup_trials=5,
                                                                   n_warmup_steps=24,
                                                                   interval_steps=6))
    log_file = args.log_dir + 'optimization_logs.pkl'
    #if os.path.isfile(log_file):
    #    study = joblib.load(log_file)

    study.optimize(objective, n_trials=args.n_trials, gc_after_trial=True)
    joblib.dump(study, log_file)




if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument('--train-dir', type=str, default='../data/database_train.csv')
    parser.add_argument('--test-dir', type=str, default='../data/database_test.csv')
    parser.add_argument('--eval-dir', type=str, default='../data/valdataset_3_3_3.csv')
    parser.add_argument('-b', '--batch-size', default=128, type=int, metavar='BS',help='number of batch size (default: 32)')
    parser.add_argument('--log-dir', type=str, default='UC3/results/optuna/')
    parser.add_argument('--total-epoch', type=int, default=5)
    parser.add_argument('--num-workers', type=int, default=8)
    parser.add_argument('--embedding-dim', type=int, nargs='+', default=[32])
    parser.add_argument('-p', '--is-permute', dest='is_permute', action='store_true')
    parser.add_argument('-d', '--is-debug', dest='is_debug', action='store_true')
    parser.add_argument('-s', '--step-save', dest='step_save', action='store_true')
    parser.add_argument('-n', '--is-normalize', dest='is_normalize', action='store_true')
    parser.add_argument('-t', '--is-detrend', dest='is_detrend', action='store_true')
    parser.add_argument('-g', '--is-gradient', dest='is_gradient', action='store_true')

    parser.add_argument('--h-tol', type=float, default=1e-4)
    parser.add_argument('--rho-max', type=float, default=1e16)
    parser.add_argument('--max-iter', type=int, default=5)
    parser.add_argument('--rho-init', type=float, default=1.0)
    parser.add_argument('--alpha-init', type=float, default=0.0)

    parser.add_argument('--n-trials', type=int, default=1)

    parser.add_argument('--method', type=str, nargs='+', default=["ganf"])  #

    parser.add_argument('--smooth_window', type=int, nargs='+', default=[1, 3, 5, 7, 9, 11,13])#
    parser.add_argument('--learning-rate', type=float, nargs='+', default=[1e-3, 5e-4, 1e-4, 5e-5])
    parser.add_argument('--penalty-rate', type=float, nargs='+', default=[0,1e-2,5e-3, 1e-3, 5e-4]) #1e-2,5e-3, 1e-3, 5e-4,

    args = parser.parse_args()

    main(args)
 

