"""
This code is generated by Ridvan Salih KUZU
LAST EDITED:  20.02.2023
ABOUT SCRIPT:
It defines training and evaluation functions
"""

import sys
sys.path.append("../..")
#import sys; sys.argv=['']; del sys
from joblib import Parallel, delayed
from tqdm import tqdm
import optuna
from optuna.samplers import TPESampler
import argparse
import warnings
import joblib
import gc
import numpy as np
import os
from represent.tools.utils import anomaly_score_reports,frame_anomaly_table
from represent.datamodules.uc3_datamodule import Dataloader
from represent.models.uc3_benchmark import rrcf
from represent.models.uc3_benchmark.maxdiv import maxdiv

warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)

warnings.filterwarnings("ignore", category=UserWarning)







def test_evaluate(method,data_loader, output_file_template,step="final"):
    '''
           THIS FUNCTION MANAGES THE EXECUTION OF TRAINING AND VALIDATION EPOCHS
           :param data_loader: data loader
           :param loss_type: l1 or dtw to select the loss function
           :param output_file_template: file name template to save the fine-tuned uc1_resnet_base.py and inference statistics
           :return:
       '''


    df_test=eval_epoch(method,data_loader.get_data_loader('test'))
    df_test.to_csv("{}_prediction_results_organic_{}.csv".format(output_file_template,step),index=False)

    df_eval = eval_epoch(method,data_loader.get_data_loader('eval'))
    df_eval.to_csv("{}_prediction_results_synthetic_{}.csv".format(output_file_template,step),index=False)

    df_report, u, acc = anomaly_score_reports(df_test, df_eval, min_percentile=80, step_resolution=1,per_building=False)
    df_report.to_csv("{}_acc_{}_u_{}_prediction_report_{}.csv".format(output_file_template,acc,u,step), index=False)

    return acc


def eval_epoch(method,data_loader):
    '''
        THIS FUNCTION MANAGES THE EXECUTION OF TRAINING AND VALIDATION EPOCHS
        :param data_loader: data loader for evaluation file
        :return:
    '''

    loss_list=[]
    unique_list = []
    footprint_list = []
    anomaly_list = []
    tree = rrcf.RCTree()
    tqdm_object = tqdm(data_loader, total=len(data_loader))
    for data_batch, permuted_data_batch, unique_id_batch, foot_batch, anomaly_batch in tqdm_object:
        if method == "maxdiv":
            loss = maxdiv_anomaly(data_batch.cpu().numpy())
            loss_list.append(loss)
        elif method == "rrcf":
            tree = rrcf_anomaly(tree, data_batch.cpu().numpy(), unique_id_batch.cpu().numpy())

        unique_list.append(unique_id_batch.detach().cpu().numpy())
        footprint_list.append(foot_batch.detach().cpu().numpy())
        anomaly_list.append(anomaly_batch.detach().cpu().numpy())

    if method=="maxdiv":
        loss_list=np.concatenate(loss_list, 0)
        loss_list=np.expand_dims(np.expand_dims(loss_list,-1),-1)

    if method == "rrcf":
        loss_list=[np.asarray([tree.codisp(leaf) for leaf in tree.leaves])]
        loss_list = np.concatenate(loss_list, 0)
        loss_list=np.expand_dims(np.expand_dims(loss_list,-1),-1)

    df=frame_anomaly_table(loss_list, np.concatenate(unique_list), np.concatenate(footprint_list),  np.concatenate(anomaly_list))

    return df

def maxdiv_anomaly(data_array):
    ll=np.zeros(len(data_array))
    for i in range(data_array.shape[0]):
        score = maxdiv.maxdiv(np.transpose(data_array[i, :, :], (1, 0)))[0][-1]
        ll[i]=score
    return ll

def maxdiv_anomaly_p(data_array):
    ll=np.zeros(len(data_array))
    def _parallel_run(i):
        score = maxdiv.maxdiv(np.transpose(data_array[i, :, :], (1, 0)))[0][-1]
        return score,i
    output=Parallel(n_jobs=-1)(delayed(_parallel_run)(k) for k in range(data_array.shape[0]))
    for score,idx in output:
        ll[idx]=score
    return ll

def rrcf_anomaly(tree,data_array,id_array):
    for idx in range(data_array.shape[0]):
        tree.insert_point(np.transpose(data_array[idx, :, :], (1, 0)),id_array[idx])
    return tree


def main(args):

    if not os.path.exists(args.log_dir):
        os.makedirs(args.log_dir,exist_ok=True)

    MANUAL_SEED=42
    LABEL_NAMES = ["trend", "noise", "step"]
    np.random.seed(MANUAL_SEED)

    def objective(trial):
        '''
            THIS FUNCTION DEFINES THE OBJECTIVE FUNCTION FOR BAYESIAN HYPERPARAMETER TUNING
            :param trial: trial object of bayesian optimization
            :return: returns weighted F1 score to be maximized
        '''

        gc.collect()

        smooth_window = trial.suggest_categorical('smooth_window', args.smooth_window)
        method = trial.suggest_categorical('method', args.method)

        is_normalize = args.is_normalize #trial.suggest_categorical('is_normalize', [True])
        is_detrend = args.is_detrend#trial.suggest_categorical('is_detrend', [False])
        is_permute=args.is_permute
        is_debug = args.is_debug
        is_gradient=args.is_gradient

        print(f"INFO: Trial number: {trial.number}")
        print(f"INFO: Smooth window: {smooth_window}")
        print(f"INFO: Is normalize: {is_normalize}")
        print(f"INFO: Is detrend: {is_detrend}")
        print(f"INFO: Classical ML: {method}")
        print(f"INFO: Data Permutation: {is_permute}")
        print(f"INFO: Data Gradient: {is_gradient}")
        print(f"INFO: DEBUG mode: {is_debug}\n")

        data_loader = Dataloader(args.train_dir,
                                 args.test_dir,
                                 args.eval_dir,
                                 split=0.25,
                                 batch_size=args.batch_size,
                                 window_length=smooth_window,
                                 num_workers=args.num_workers,
                                 is_normalize=is_normalize,
                                 is_detrend=is_detrend,
                                 is_permute=is_permute,
                                 is_gradient=is_gradient,
                                 is_debug=is_debug,
                                 seed=MANUAL_SEED)


        output_file_template = '{}sw_{}_in_{}_id_{}_ip_{}_ig_{}_me_{}_tn_{}'.format(args.log_dir,smooth_window,is_normalize,is_detrend,is_permute,is_gradient,method, trial.number)

        acc=test_evaluate(method,data_loader, output_file_template)
        print(f"INFO: Accuracy @ 80 percentile: {acc}\n")
        return acc

    study = optuna.create_study(sampler=TPESampler(), direction='maximize',
                                pruner=optuna.pruners.MedianPruner(n_startup_trials=5,
                                                                   n_warmup_steps=24,
                                                                   interval_steps=6))
    log_file = args.log_dir + 'optimization_logs.pkl'
    #if os.path.isfile(log_file):
    #    study = joblib.load(log_file)

    study.optimize(objective, n_trials=args.n_trials, gc_after_trial=True)
    joblib.dump(study, log_file)




if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument('--train-dir', type=str, default='data/database_train.csv')
    parser.add_argument('--test-dir', type=str, default='data/database_test.csv')
    parser.add_argument('--eval-dir', type=str, default='data/valdataset_3_3_3.csv')
    parser.add_argument('-b', '--batch-size', default=4, type=int, metavar='BS',help='number of batch size (default: 32)')
    parser.add_argument('--log-dir', type=str, default='UC3/results/optuna/')
    parser.add_argument('--num-workers', type=int, default=8)
    parser.add_argument('-p', '--is-permute', dest='is_permute', action='store_true')
    parser.add_argument('-d', '--is-debug', dest='is_debug', action='store_true')
    parser.add_argument('-n', '--is-normalize', dest='is_normalize', action='store_true')
    parser.add_argument('-t', '--is-detrend', dest='is_detrend', action='store_true')
    parser.add_argument('-g', '--is-gradient', dest='is_gradient', action='store_true')
    parser.add_argument('--method', type=str, nargs='+', default=["maxdiv","rrcf"])  #
    parser.add_argument('--n-trials', type=int, default=1)
    parser.add_argument('--smooth_window', type=int, nargs='+', default=[1, 3, 5, 7, 9, 11,13])#


    args = parser.parse_args()

    main(args)
 

